{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c76b852",
   "metadata": {},
   "source": [
    "# Path integral Monte Carlo\n",
    "The programme implements the path integral Monte Carlo procedure to diagonalize the anharmonic oscillator, whose path integral is given by:\n",
    "\n",
    "$$\n",
    "\\left<x_f\\left|\\ e^{-iHt}\\ \\right|x_i\\right>\\ =\\int_{x(0)=x_i}^{x(t)=x_f} \\mathcal{D}x\\ e^{iS[x]}\\qquad\\qquad\n",
    "S[x]=\\int_0^t dt\\left[\\frac{1}{4}\\dot{x}^2-\\left(x^2-\\eta^2\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Input parameters, with default values:\n",
    "- potential parameters: $\\lambda=1$, $\\eta=1.4$\n",
    "- mass of the particle: $m=0.5$\n",
    "- number of lattice points: $N=800$\n",
    "- lattice spacing: $a=0.05$\n",
    "- number of Metropolis configurations: $N_{sweeps}=100$\n",
    "- number of sweeps between each measurement: $N_{cor}=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2a0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eee476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = \n",
      "eta = \n",
      "m = \n"
     ]
    }
   ],
   "source": [
    "# Parameters of the potential\n",
    "Lambda = float(input(\"lambda = \") or 1);\n",
    "eta = float(input(\"eta = \") or 1.4);\n",
    "m = float(input(\"m = \") or 1/2);\n",
    "hbar = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a72816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = \n",
      "a = \n",
      "Nsweeps = \n",
      "Ncorrelation = \n"
     ]
    }
   ],
   "source": [
    "# Lattice parameters\n",
    "N = int(input(\"N = \") or 800);\n",
    "a = float(input(\"a = \") or 0.05);\n",
    "Nsweeps = int(input(\"Nsweeps = \") or 100);\n",
    "Ncor = int(input(\"Ncorrelation = \") or 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08d542",
   "metadata": {},
   "source": [
    "## Exact diagonalization\n",
    "We repeat here the exact diagonalization procedure, which will be used for comparison in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c4d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrid = \n",
      "xmin = \n",
      "xmax = \n"
     ]
    }
   ],
   "source": [
    "Ngrid = int(input(\"Ngrid = \") or 800);\n",
    "xmin = float(input(\"xmin = \") or -3);\n",
    "xmax = float(input(\"xmax = \") or 3);\n",
    "\n",
    "# Potential\n",
    "xvec = np.linspace(xmin, xmax, Ngrid);     # Vector spanning from xmin to xmax with Ngrid points\n",
    "Vx = Lambda*(xvec**2-eta**2)**2;\n",
    "\n",
    "# Kinetic energy discretization\n",
    "dx = np.diff(xvec).mean();     # Grid resolution\n",
    "diag = -2*np.ones(Ngrid);      # -2f(x)\n",
    "offdiag = np.ones(Ngrid-1);    # f(x+dx) and f(x-dx)\n",
    "\n",
    "d2grid = np.mat(np.diag(diag,0) + np.diag(offdiag,-1) + np.diag(offdiag,1))/dx**2;     # Discretized second derivative\n",
    "\n",
    "# Avoid problems at the edge of the grid\n",
    "d2grid[0,:] = 0;\n",
    "d2grid[Ngrid-1,:] = 0;\n",
    "\n",
    "Ekin = -hbar**2/(2*m)*d2grid;\n",
    "\n",
    "# Potential energy discretization\n",
    "Epot = np.mat(np.diag(Vx, 0))\n",
    "\n",
    "# Total Hamiltonian diagonalization\n",
    "H = Ekin + Epot;\n",
    "w, v = LA.eig(H);\n",
    "\n",
    "sortinds = np.argsort(w);     # Returns the indices that would sort the array w\n",
    "EigVals = w[sortinds];        # Eigenvalues sorting\n",
    "EigVecs = v[:,sortinds];      # Corresponding eigenvectors sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d208af",
   "metadata": {},
   "source": [
    "## Monte Carlo method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd7940",
   "metadata": {},
   "source": [
    "### Evaluating the action\n",
    "The action in euclidean time $\\beta=it$ is given by:\n",
    "\n",
    "$$\n",
    "S_E[x]=\\int_0^\\beta d\\tau \\left[\\frac{1}{4}\\dot{x}^2+\\left(x^2-\\eta^2\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "We discretize the euclidean time coordinate as: $\\tau_j=ja$, with $j=1,\\dots,N$, so that the action becomes:\n",
    "\n",
    "$$\n",
    "S=\\sum_{j=1}^N \\left[\\frac{1}{4a}\\left(x_j-x_{j-1}\\right)^2+a\\left(x_j^2-\\eta^2\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "where $x_j\\equiv x(\\tau_j)$. Since only few terms of the action change when $x_j$ is updated in the Metropolis algorithm, the following function computes only these terms, as $\\Delta S = S'-S = S'_j-S_j$ when $x_j$ is changed, where we denote by $S_j$ the action with only $x_j$-dependent terms, and the prime means updated:\n",
    "\n",
    "$$\n",
    "S_j=\\frac{1}{4a}\\left[\\left(x_j-x_{j-1}\\right)^2+\\left(x_{j+1}-x_j\\right)^2\\right]+a\\left(x_j^2-\\eta^2\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221c3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that evaluates the action of a given configuration considering only j-dependent terms\n",
    "def S(j, x):\n",
    "    if j == N-1:\n",
    "        Action = ((x[j]-x[j-1])**2 + (x[0]-x[j])**2)/(4*a) + a*(x[j]**2-eta**2)**2;       # Periodic boundary conditions\n",
    "    else:\n",
    "        Action = ((x[j]-x[j-1])**2 + (x[j+1]-x[j])**2)/(4*a) + a*(x[j]**2-eta**2)**2;\n",
    "    return Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e71c14",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "We can choose wether to start with a cold configuration $\\{x_i\\}^{(0)}=\\{0\\}$ or with a hot one $\\{x_i\\}^{(0)}=\\{r_i\\}$, where $r_i$ is a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f320d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type C for cold start, H for hot start: C\n"
     ]
    }
   ],
   "source": [
    "# Generating the initial configuration\n",
    "x = np.zeros(N);\n",
    "\n",
    "while True:\n",
    "    choice = input(\"Type C for cold start, H for hot start: \")\n",
    "    if choice == \"C\":                          # Cold start configuration\n",
    "        break\n",
    "    elif choice == \"H\":\n",
    "        for i in range(N):\n",
    "            x[i] = x[i] + random.random();     # Hot start configuration\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3ba57",
   "metadata": {},
   "source": [
    "### Metropolis algorithm\n",
    "The Metropolis method generates an ensemble of configurations $\\{x_i\\}^{(k)}$, where $i=1,\\dots,N$ labels the lattice points and $k=1,\\dots,N_{sweeps}$ the various configurations. The configurations are generated via a trial update $x_i^{(k+1)}=x_i^{(k)}+\\delta x$ performed for every lattice site, where $\\delta x$ is for instance a Gaussian random number with the width of the distribution $\\varepsilon$ adjusted such that the average acceptance rate for the trial updates is around $50\\%$. The trial update is accepted with probability:\n",
    "\n",
    "$$\n",
    "P\\left(x_i^{(k)}\\rightarrow x_i^{(k+1)}\\right)=\\mbox{min}\\left\\{e^{-\\Delta S}, 1\\right\\}\n",
    "$$\n",
    "\n",
    "A similar algorithm can be used to find the nearest classical solution to a given quantum one (the so called $\\textit{instanton solution}$), by accepting only the updates that lower the action, $\\Delta S<0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9fa8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = \n"
     ]
    }
   ],
   "source": [
    "eps = float(input(\"epsilon = \") or 0.5);\n",
    "\n",
    "# Function that updates x according to Metropolis algorithm\n",
    "def update(y):\n",
    "    for j in range(N):\n",
    "        old_y = y[j];                                    # Save the original values\n",
    "        old_Sj = S(j,y);\n",
    "        y[j] = y[j] + random.gauss(0, eps);              # Update x[j]\n",
    "        dS = S(j,y) - old_Sj;                            # Change in action\n",
    "        if math.exp(-dS) < random.gauss(0, eps):         # Metropolis acceptance criterium\n",
    "            y[j] = old_y;                                # Restore the old value in case the condition is not satisfied\n",
    "            \n",
    "# Function that updates x cooling the system\n",
    "def cooling(y):\n",
    "    for j in range(N):\n",
    "        old_y = y[j];                                    # Save the original values\n",
    "        old_Sj = S(j,y);\n",
    "        y[j] = y[j] + random.gauss(0, eps);              # Update x[j]\n",
    "        dS = S(j,y) - old_Sj;                            # Change in action\n",
    "        if dS > 0:                                       # Cooling condition\n",
    "            y[j] = old_y;                                # Restore the old value in case the condition is not satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f187f0e",
   "metadata": {},
   "source": [
    "#### Sample path\n",
    "The following code returns an example of euclidean path, obtained after $N_{path}=1000$ thermalization sweeps; the path is then cooled through $N_{cool}=200$ thermalization sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Npath = int(input(\"Npath = \") or 1000);\n",
    "Ncool = int(input(\"Ncool = \") or 200);\n",
    "\n",
    "def Path(y):\n",
    "    for j in tqdm(range(Npath), leave = False):           # Updating the path through Npath steps\n",
    "        update(y)\n",
    "    return y\n",
    "\n",
    "x = Path(x);\n",
    "tau_vec = np.linspace(0, N*a, N);          # Total euclidean time obtained from lattice spacing a and from N\n",
    "\n",
    "# Plotting the euclidean path\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(tau_vec, x, label = 'Monte Carlo');\n",
    "ax.set_xlabel('$\\\\tau$');\n",
    "ax.set_ylabel('$x(\\\\tau)$');\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([min(x), max(x)])\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Euclidean path for $\\eta =$'+str(eta));\n",
    "\n",
    "def CooledPath(y):\n",
    "    for k in tqdm(range(Ncool), leave = False):           # Cooling the path through Ncool steps\n",
    "        cooling(y)\n",
    "    return y\n",
    "\n",
    "x_cooled = CooledPath(x);\n",
    "\n",
    "# Plotting the cooled path\n",
    "ax.plot(tau_vec, x_cooled, '-', label = 'Cooled');\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82235ead",
   "metadata": {},
   "source": [
    "We see that for small $\\tau$ the system is controlled by the oscillation time $\\tau_{osc}$, while for large $\\tau$ it is governed by the tunneling time $\\tau_{tun}$. We can estimate:\n",
    "\n",
    "$$\n",
    "\\tau_{osc}\\sim \\frac{1}{4\\eta}\\qquad\\mbox{and}\\qquad \\tau_{tun}\\sim \\mbox{exp}\\left(-\\frac{4}{3}\\eta^3\\right)\n",
    "$$\n",
    "\n",
    "and in order to have a reliable simulation we should check that the lattice spacing is small compared to $\\tau_{osc}$ and the total length of the lattice $Na$ is much larger than the tunneling time:\n",
    "$$\n",
    "a\\ll \\tau_{osc}\\qquad\\mbox{and}\\qquad \\tau_{tun}\\ll Na\n",
    "$$\n",
    "In our case we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3022ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a = \" + str(a) + \"           tau_osc = \" + str(round(1/(4*eta), 3)))\n",
    "print(\"tau_tun = \" + str(round(math.exp(-4*eta**3/3), 3)) + \"    N*a = \" + str(N*a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78d0e9",
   "metadata": {},
   "source": [
    "### Correlation functions\n",
    "The two point correlation function of order $t$ is defined as follows:\n",
    "\n",
    "$$\n",
    "G_t^{(2)}(x, \\Delta\\tau)\\equiv \\langle [x(\\tau)]^t [x(\\tau+\\Delta\\tau)]^t\\rangle \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{j\\in J}x_i^t x_j^t\n",
    "$$\n",
    "\n",
    "where $J=\\left\\{j: 0\\leq j\\leq N, j-i=\\Delta\\tau\\ \\mbox{mod } N\\right\\}$ and $x_i = x(\\tau_i)$, $\\tau_i = i\\Delta\\tau$. In the following we will consider $t=1, 2, \\dots, N_{cf}$ with $N_{cf}=3$ by default. Quantum mechanical averages are computed by averaging observables over many configurations:\n",
    "\n",
    "$$\n",
    "\\langle\\mathcal{O}\\rangle = \\lim_{N_{config}\\to \\infty}\\frac{1}{N_{config}}\\sum_{k=1}^{N_{config}}\\mathcal{O}^{(k)}\n",
    "$$\n",
    "\n",
    "The method allows to estimate the error in the measurement of $\\left<\\mathcal{O}\\right>$ from statistical fluctuations, under the assumption that the different configurations are statistically independent (which is to be verified by computing the auto-correlation time in successive measurements, and can be tuned by $N_{cor}$):\n",
    "\n",
    "$$\n",
    "\\Delta\\left<\\mathcal{O}\\right>=\\sqrt{\\frac{\\left<\\mathcal{O}^2\\right>-\\left<\\mathcal{O}\\right>^2}{N_{sweeps}}}\n",
    "$$\n",
    "\n",
    "We will therefore need to compute the mean value $\\left<\\mathcal{O}\\right>$ as well, and to consider one-point correlation functions as well:\n",
    "\n",
    "$$\n",
    "G_t^{(1)}(x)\\equiv \\langle [x(\\tau)]^t \\rangle \\approx \\frac{1}{N}\\sum_{i=1}^N x_i^t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872023ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncf = int(input(\"N of correlation functions = \") or 3);\n",
    "\n",
    "# Function that computes the two-point correlation function of order t\n",
    "def computeG2(x, dtau, t):\n",
    "    g = 0;                                     # Initialize the value of the correlation function\n",
    "    for j in range(N):\n",
    "        g = g + (x[j]**t)*(x[(j+dtau)%N]**t);  # Add all the terms satisfying the above condition for J\n",
    "    return g/N\n",
    "\n",
    "# Function that computes the Monte Carlo average\n",
    "def MCaverage(x):\n",
    "    G = np.zeros((Nsweeps, N, Ncf))            # Initialize the value of the two-point correlation function\n",
    "    G1 = np.zeros((Nsweeps, N, Ncf))           # Initialize the value of the one-point correlation function\n",
    "    \n",
    "    for j in range(5*Ncor):                    # Initial thermalization\n",
    "        update(x)\n",
    "    for k in tqdm(range(Nsweeps), leave = False):\n",
    "        for j in range(Ncor):                  # Thermalization steps between each measurement\n",
    "            update(x)\n",
    "        for t in range(Ncf):                   # Evaluate correlation functions of order t = 1, 2, ..., Ncf\n",
    "            for n in range(N):\n",
    "                G[k, n, t] = computeG2(x, n, t+1);\n",
    "                G1[k, n, t] = x[n]**t;\n",
    "\n",
    "    avg_G = np.zeros((N, Ncf));                # Initialize the value of the average\n",
    "    avg_G1 = avg_G;                            # (both for one- and two-point correlation functions)\n",
    "    \n",
    "    for n in range(N):                         # Compute the Monte Carlo average for each order t\n",
    "        for t in range(Ncf):\n",
    "            for k in range(Nsweeps):\n",
    "                avg_G[n, t] = avg_G[n, t] + G[k, n, t];          # Two-point correlation function\n",
    "                avg_G1[n, t] = avg_G1[n, t] + G1[k, n, t];       # One-point correlation function\n",
    "    avg_G = avg_G / Nsweeps;\n",
    "    avg_G1 = avg_G1 / Nsweeps;\n",
    "        \n",
    "    return [avg_G, avg_G1]\n",
    "\n",
    "\n",
    "# Computing the first Ncf correlation functions\n",
    "[G_two, G_one] = MCaverage(x)\n",
    "\n",
    "deltaG = np.sqrt(np.abs(G_two-np.power(G_one, 2)) / Nsweeps);          # Statistical error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe02b8",
   "metadata": {},
   "source": [
    "#### Comparison with exact diagonalization\n",
    "The correlation functions can be also computed using exact diagonalization, in order to see wether the two different methods give the same result. Letting $O(\\tau)=x(\\tau)^t$, we have that the two-point correlation functions $G_t^{(2)}(x,\\tau)=\\left<O(0)O(\\tau)\\right>$ can be computed by inserting a complete set of states in the expectation value:\n",
    "\n",
    "$$\n",
    "G_t^{(2)}(x,\\tau)=\\sum_{n}\\left|\\left<0|O(0)|n\\right>\\right|^2\\ e^{-(E_n-E_0)\\tau}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that evaluates the braket in the above expression for G\n",
    "def Compute_xvalues(v):\n",
    "    w = 4*eta;                       # Frequency of a corresponding harmonic oscillator\n",
    "    rho1 = np.zeros(N);              # Initialize the density of states\n",
    "    rho2 = np.zeros(N);\n",
    "    rho3 = np.zeros(N);\n",
    "    \n",
    "    for n in tqdm(range(N), leave = False):         # Loop over all lattice points\n",
    "        cn = 0;\n",
    "        dn = 0;\n",
    "        en = 0;\n",
    "        \n",
    "        for k in range(N):           # Loop over all eigenvalues\n",
    "            km3 = max(k-3, 0);       # Conditions to ensure that the square roots exist\n",
    "            km2 = max(k-2, 0);\n",
    "            km1 = max(k-1, 0);\n",
    "            kp1 = min(k+1, N-1);\n",
    "            kp2 = min(k+2, N-1);\n",
    "            kp3 = min(k+3, N-1);\n",
    "            \n",
    "            # Expressions coming from analytic computation of the braket\n",
    "            cn = cn + ( np.sqrt(k)*v[km1, 0] + np.sqrt(k+1)*v[kp1, 0] )*v[k, n];\n",
    "            dn = dn + ( np.sqrt(k*(k-1))*v[km2, 0] + (2*k+1)*v[k, 0] + np.sqrt((k+1)*(k+2))*v[kp2, 0] )*v[k, n];\n",
    "            en = en + ( np.sqrt(k*(k-1)*(k-2))*v[km3, 0] + 3*k*np.sqrt(k)*v[km1, 0] + 3*(k+1)*np.sqrt(k+1)*v[kp1, 0] \\\n",
    "                    + np.sqrt((k+1)*(k+2)*(k+3))*v[kp3, 0] )*v[k, n];\n",
    "            \n",
    "        rho1[n] = 1/(m*w)/2*cn**2;\n",
    "        rho2[n] = 1/(m*w)**2/4*dn**2;\n",
    "        rho3[n] = 1/(m*w)**3/8*en**2;\n",
    "        \n",
    "    return rho1, rho2, rho3\n",
    "            \n",
    "\n",
    "# Function that evaluates the two-point correlation function from exact diagonalization\n",
    "def ComputeG_exact():\n",
    "    G_exact = np.zeros((N, 3));                        # Here Ncf = 3 for simplicity\n",
    "    rho1, rho2, rho3 = Compute_xvalues(EigVecs);\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(N):\n",
    "                G_exact[n, 0] = G_exact[n, 0] + rho1[k]**2 * np.exp(-(EigVals[k]-EigVals[0])*tau_vec[n]);\n",
    "                G_exact[n, 1] = G_exact[n, 1] + rho2[k]**2 * np.exp(-(EigVals[k]-EigVals[0])*tau_vec[n]);\n",
    "                G_exact[n, 2] = G_exact[n, 2] + rho3[k]**2 * np.exp(-(EigVals[k]-EigVals[0])*tau_vec[n]);\n",
    "                \n",
    "    return G_exact\n",
    "\n",
    "G_exact = ComputeG_exact();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation functions\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for i in range(Ncf):\n",
    "    ax.plot(tau_vec, G_two[:,i], 'o', label = 't = '+str(i+1))\n",
    "    plt.errorbar(tau_vec, G_two[:,i], yerr = deltaG[:,i], xerr = None, fmt = 'None', ecolor = 'black')\n",
    "    \n",
    "    #ax. plot(tau_vec, G_exact[:,i])\n",
    "\n",
    "ax.set_xlabel('$\\\\tau$');\n",
    "ax.set_ylabel('$G_t(x,\\Delta\\\\tau)$');\n",
    "plt.xlim([0, 1.5]);\n",
    "plt.ylim([np.min(G_two), np.max(G_two)])\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Correlation functions of order $t$ for $\\eta =$'+str(eta));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3abb5",
   "metadata": {},
   "source": [
    "#### Probability distribution\n",
    "The probability distribution $|\\psi(x)|^2$ computed exactly via diagonalization can be compared with the number of counts coming from the Monte Carlo simulation, upon scaling one of the two by a factor of $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98841d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_squared = np.power(np.abs(EigVecs[:,0]), 2)            # Computing the square modulus of the wavefunction\n",
    "norm = np.sum(EigVecs[:,0])                                # Normalization\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.hist(x, 30, label = 'Monte Carlo');\n",
    "ax.plot(xvec, N*norm*psi_squared, label = 'Exact')         # Exact result from diagonalization, for comparison\n",
    "ax.set_xlabel('$x$');\n",
    "ax.set_ylabel('Number of counts');\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Probability distribution for $\\eta =$'+str(eta));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51016138",
   "metadata": {},
   "source": [
    "#### Logarithmic derivative of the correlation function\n",
    "The energy levels can be obtained from the logarithmic derivative of the correlation function,\n",
    "\n",
    "$$\n",
    "C_t(\\tau)=-\\frac{d}{d\\tau}\\log{G_t^{(2)}}(\\tau) = \\frac{\\sum_n (E_n-E_0)|\\left<0|O(0)|n\\right>|^2 e^{-(E_n-E_0)\\tau}}{\\sum_n |\\left<0|O(0)|n\\right>|^2 e^{-(E_n-E_0)\\tau}}\n",
    "$$\n",
    "\n",
    "which in the limit $\\tau\\to \\infty$ converges to the energy splitting between the ground state and the first excited state that has a non-zero transition amplitude $\\left<0|O(0)|n\\right>$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the logarithmic derivative of a vector\n",
    "# (intended as a function of tau)\n",
    "def logder(x):\n",
    "    logx = np.log(x);\n",
    "    logder = -np.gradient(logx, a);\n",
    "    return logderc\n",
    "\n",
    "# Plotting the logarithmic derivative of the correlation functions\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for i in range(Ncf):\n",
    "    ax.plot(tau_vec, logder(np.abs(G_two[:,i])), 'o', label = 't = '+str(i+1))\n",
    "    plt.errorbar(tau_vec, logder(np.abs(G_two[:,i])), yerr = deltaG[:,i]/np.abs(G_two[:,i]), \\\n",
    "                                                                                xerr = None, fmt = 'None', ecolor = 'black')\n",
    "    #ax. plot(tau_vec, G_exact[:,i])\n",
    "\n",
    "# Even-order correlation function with constant value subtracted\n",
    "ax.plot(tau_vec, logder(np.abs(G_two[:,1] - np.power(G_one[:,1], 2))), 'o', label = 't = 2 (sub)')\n",
    "plt.errorbar(tau_vec, logder(np.abs(G_two[:,1] - np.power(G_one[:,1], 2))), \\\n",
    "                yerr = (deltaG[:,1] + 2*deltaG[:,1])/(np.abs(G_two[:,1] - np.power(G_one[:,1], 2))), \\\n",
    "                xerr = None, fmt = 'None', ecolor = 'black')\n",
    "\n",
    "ax.set_xlabel('$\\\\tau$');\n",
    "ax.set_ylabel('$G_t(x,\\Delta\\\\tau)$');\n",
    "plt.xlim([0, 1.5])\n",
    "plt.ylim([-5, 20])\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Logarithmic derivative of correlation functions for $\\eta =$'+str(eta));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f770972",
   "metadata": {},
   "source": [
    "## Cooled Monte Carlo\n",
    "We can repeat the same computations above, starting now from cooled configurations, in order to see what are the differences among the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b87ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the Monte Carlo average\n",
    "def MCaverage_cooled(x):\n",
    "    G = np.zeros((Nsweeps, N, Ncf))            # Initialize the value of the two-point correlation function\n",
    "    G1 = np.zeros((Nsweeps, N, Ncf))           # Initialize the value of the one-point correlation function\n",
    "    \n",
    "    for j in range(5*Ncor):                    # Initial thermalization\n",
    "        update(x)\n",
    "    for k in tqdm(range(Nsweeps), leave = False):\n",
    "        for j in range(Ncor):                  # Thermalization steps between each measurement\n",
    "            update(x)\n",
    "        for j in range(Ncool):                 # Cooling steps before measurement\n",
    "            cooling(x)\n",
    "        for t in range(Ncf):                   # Evaluate correlation functions of order t = 1, 2, ..., Ncf\n",
    "            for n in range(N):\n",
    "                G[k, n, t] = computeG2(x, n, t+1);\n",
    "                G1[k, n, t] = x[n]**t;\n",
    "\n",
    "    avg_G = np.zeros((N, Ncf));                # Initialize the value of the average\n",
    "    avg_G1 = avg_G;                            # (both for one- and two-point correlation functions)\n",
    "    \n",
    "    for n in range(N):                         # Compute the Monte Carlo average for each order t\n",
    "        for t in range(Ncf):\n",
    "            for k in range(Nsweeps):\n",
    "                avg_G[n, t] = avg_G[n, t] + G[k, n, t];          # Two-point correlation function\n",
    "                avg_G1[n, t] = avg_G1[n, t] + G1[k, n, t];       # One-point correlation function\n",
    "    avg_G = avg_G / Nsweeps;\n",
    "    avg_G1 = avg_G1 / Nsweeps;\n",
    "        \n",
    "    return [avg_G, avg_G1]\n",
    "\n",
    "\n",
    "# Computing the first Ncf correlation functions\n",
    "[G_twoC, G_oneC] = MCaverage_cooled(x)\n",
    "\n",
    "deltaGC = np.sqrt(np.abs(G_twoC-np.power(G_oneC, 2)) / Nsweeps);          # Statistical error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation functions\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for i in range(Ncf):\n",
    "    ax.plot(tau_vec, G_twoC[:,i], 'o', label = 't = '+str(i+1))\n",
    "    plt.errorbar(tau_vec, G_twoC[:,i], yerr = deltaGC[:,i], xerr = None, fmt = 'None', ecolor = 'black')\n",
    "    \n",
    "    #ax. plot(tau_vec, G_exact[:,i])\n",
    "\n",
    "ax.set_xlabel('$\\\\tau$');\n",
    "ax.set_ylabel('$G_t(x,\\Delta\\\\tau)$');\n",
    "plt.xlim([0, 1.5]);\n",
    "plt.ylim([np.min(G_twoC), np.max(G_twoC)]);\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Cooled correlation functions of order $t$ for $\\eta =$'+str(eta));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82ffb5",
   "metadata": {},
   "source": [
    "### Logarithmic derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c83886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the logarithmic derivative of the correlation functions\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for i in range(Ncf):\n",
    "    ax.plot(tau_vec, logder(np.abs(G_twoC[:,i])), 'o', label = 't = '+str(i+1))\n",
    "    plt.errorbar(tau_vec, logder(np.abs(G_twoC[:,i])), yerr = deltaG[:,i]/np.abs(G_twoC[:,i]), \\\n",
    "                                                                                xerr = None, fmt = 'None', ecolor = 'black')\n",
    "    #ax. plot(tau_vec, G_exact[:,i])\n",
    "\n",
    "# Even-order correlation function with constant value subtracted\n",
    "ax.plot(tau_vec, logder(np.abs(G_twoC[:,1] - np.power(G_oneC[:,1], 2))), 'o', label = 't = 2 (sub)')\n",
    "plt.errorbar(tau_vec, logder(np.abs(G_twoC[:,1] - np.power(G_oneC[:,1], 2))), \\\n",
    "                yerr = (deltaGC[:,1] + 2*deltaGC[:,1])/(np.abs(G_twoC[:,1] - np.power(G_oneC[:,1], 2))), \\\n",
    "                xerr = None, fmt = 'None', ecolor = 'black')\n",
    "\n",
    "ax.set_xlabel('$\\\\tau$');\n",
    "ax.set_ylabel('$G_t(x,\\Delta\\\\tau)$');\n",
    "plt.xlim([0, 1.5])\n",
    "plt.ylim([-5, 20])\n",
    "plt.grid(linewidth=0.5);\n",
    "plt.title('Logarithmic derivative of cooled correlation functions for $\\eta =$'+str(eta));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88034e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
